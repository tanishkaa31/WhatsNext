{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WhatsNext - Github.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTue9MTyIgcz"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fdj_MblIiSj"
      },
      "source": [
        "df = pd.read_csv('EXPORTED CHAT FROM WHATSAPP HERE', sep = '\\n', header=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5GAIKnGInFh"
      },
      "source": [
        "df = df.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHXa9VrKIqzH"
      },
      "source": [
        "import spacy \n",
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0_Qfi1BIt2n"
      },
      "source": [
        "lines_with_tokens = []\n",
        "for line in df:\n",
        "    doc= nlp(line[0])    #to access the string of the array of df\n",
        "    tokens=[token.text for token in doc]\n",
        "    lines_with_tokens.append(tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Umt0-y-5IxPJ"
      },
      "source": [
        "i = 0\n",
        "while i<len(lines_with_tokens):\n",
        "  if len(lines_with_tokens[i])>5 and lines_with_tokens[i][5] == 'NAME OF OTHER USER':\n",
        "    del lines_with_tokens[i]\n",
        "  else:\n",
        "    i +=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZHv1H9LI5ga"
      },
      "source": [
        "len(lines_with_tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ftnn_oxiJeEC"
      },
      "source": [
        "import string\n",
        "from string import punctuation\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvGaL_fKJeqZ"
      },
      "source": [
        "words_list = ['Media', 'omitted', 'pm', 'am', 'YOUR OWN NAME']\n",
        "digits = '0123456789'\n",
        "def message_cleaning(msg):\n",
        "  msg = [char for char in msg if char not in string.punctuation and char not in digits]\n",
        "  msg = \"\".join(msg)\n",
        "  msg = [char for char in msg.split() if char not in words_list]\n",
        "  msg = \" \".join(msg)\n",
        " # msg = emoji(msg)\n",
        " # msg = [words for words in msg.split() if words.lower() not in stopwords.words('english')]\n",
        "  return msg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzbbYOisJhTm"
      },
      "source": [
        "i = 0\n",
        "while i < len(lines_with_tokens):\n",
        "  lines_with_tokens[i] = \" \".join(lines_with_tokens[i])\n",
        "  i+=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5A3ki4DJjCF"
      },
      "source": [
        "lines_with_tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ib3KkjsXJln2"
      },
      "source": [
        "i = 0\n",
        "while i < len(lines_with_tokens):\n",
        "  lines_with_tokens[i] = message_cleaning(lines_with_tokens[i])\n",
        "  i+=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bTKXwDYkJlmE"
      },
      "source": [
        "lines_with_tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zsHl8bt_Jljt"
      },
      "source": [
        "i = 0\n",
        "while i < len(lines_with_tokens):\n",
        "  lines_with_tokens[i] = lines_with_tokens[i].split()\n",
        "  i+=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sRmMmE8dJvgS"
      },
      "source": [
        "i = 0\n",
        "while i<len(lines_with_tokens):\n",
        "  for j in lines_with_tokens[i]:\n",
        "    if j.isalpha() == False:\n",
        "      lines_with_tokens[i].remove(j)\n",
        "  i += 1 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxIOO3qwJy-3"
      },
      "source": [
        "i = 0\n",
        "while i < len(lines_with_tokens):\n",
        "  if len(lines_with_tokens[i]) <= 1:\n",
        "    del lines_with_tokens[i]\n",
        "  else:\n",
        "    i+=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9XB2nwAJ1rF"
      },
      "source": [
        "i = 0\n",
        "while i<len(lines_with_tokens):\n",
        "  lines_with_tokens[i] = \" \".join(lines_with_tokens[i])\n",
        "  i+=1\n",
        "\n",
        "lines_with_tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vRbIFpmJlhk"
      },
      "source": [
        "lines_with_tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FgxjltAJ4fH"
      },
      "source": [
        "## Tokenization\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvVcaKO9JlfP"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# Tokenize the words in our headlines\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(lines_with_tokens)\n",
        "total_words = len(tokenizer.word_index) + 1 \n",
        "print('Total words: ', total_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BwQoGHM7Jlcp"
      },
      "source": [
        "tokenizer1 = Tokenizer()\n",
        "len(tokenizer1.word_index)+1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQzH5Dg0J_Rd"
      },
      "source": [
        "## Creating Sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b20akEUYJlZ3"
      },
      "source": [
        "# Convert data to sequence of tokens \n",
        "input_sequences = []\n",
        "for line in lines_with_tokens:\n",
        "   \n",
        "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "    \n",
        "    # Create a series of sequences for each line\n",
        "    for i in range(1, len(token_list)):\n",
        "        partial_sequence = token_list[:i+1]\n",
        "        input_sequences.append(partial_sequence)\n",
        "\n",
        "print(tokenizer.sequences_to_texts(input_sequences[:5]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_BxYqKaJlXK"
      },
      "source": [
        "input_sequences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZB_xg0cKFwY"
      },
      "source": [
        "## Padding Sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-FuyKsqJlS6"
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Determine max sequence length\n",
        "max_sequence_len = max([len(x) for x in input_sequences])\n",
        "\n",
        "# Pad all sequences with zeros at the beginning to make them all max length\n",
        "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
        "input_sequences[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pLnllNRJlQj"
      },
      "source": [
        "max_sequence_len"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPJjqdJyJk9L"
      },
      "source": [
        "input_sequences.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kuSbZDk1KMvM"
      },
      "source": [
        "## Creating Predictors and Target"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-m6NNKKZKMma"
      },
      "source": [
        "input_df = pd.DataFrame(input_sequences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5pFKNiarKRTj"
      },
      "source": [
        "input_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4mesA6WqKTnv"
      },
      "source": [
        "input_df.to_csv('inputs.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPPudYifKTlq"
      },
      "source": [
        "## Uncomment and run cells from here on restarting\n",
        "\n",
        "#import pandas as pd\n",
        "#input_sequences = pd.read_csv('inputs.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BiEzgeGaKTjN"
      },
      "source": [
        "input_sequences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1nG30oOKTgv"
      },
      "source": [
        "del input_sequences['Unnamed: 0']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DM99Pl7EKTcx"
      },
      "source": [
        "input_sequences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOPyxSe8KTaa"
      },
      "source": [
        "input_sequences = input_sequences.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jh57kPs-a-GN"
      },
      "source": [
        "# Predictors are every word except the last\n",
        "predictors = input_sequences[:,:-1]\n",
        "# Labels are the last word\n",
        "labels = input_sequences[:,-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sx20ORVxa-Dz"
      },
      "source": [
        "predictors.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1FIwNFva-AC"
      },
      "source": [
        "labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7U1GrsLTa9-D"
      },
      "source": [
        "from tensorflow.keras import utils\n",
        "total_words= 5880\n",
        "labels = utils.to_categorical(labels, num_classes=total_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avkAPBqwbGqv"
      },
      "source": [
        "## Creating the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-IU1xN87a97H"
      },
      "source": [
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from tensorflow.keras.models import Sequential"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bV7Hoc8Ha94b"
      },
      "source": [
        "max_sequence_len = 152\n",
        "input_len = max_sequence_len - 1 \n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(5880, 32, input_length=input_len))\n",
        "model.add(LSTM(300, return_sequences=True))     #return_sequences=True to output a 3D array as input for the subsequent layer\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(200))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(total_words, activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKzfYYEVbOGV"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i38s4szLbQU4"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfg6_VaAbQRg"
      },
      "source": [
        "model.fit(predictors, labels, epochs=250, verbose=1)    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NioYXf7KbQNn"
      },
      "source": [
        "model.save(\"Model.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ao6dYEpKbQLF"
      },
      "source": [
        "from keras.models import load_model\n",
        "# load model\n",
        "model = load_model(\"Model.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VkuVq67LbZVi"
      },
      "source": [
        "## Making Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJx7g44NbQGK"
      },
      "source": [
        "def predict_next_token(seed_text):\n",
        "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "    prediction = np.argmax(model.predict(token_list), axis=-1)\n",
        "    return prediction"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYynX-13bQCq"
      },
      "source": [
        "prediction = predict_next_token(\"INPUT TEXT HERE\")\n",
        "tokenizer.sequences_to_texts([prediction])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3V53vmmbQAL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}